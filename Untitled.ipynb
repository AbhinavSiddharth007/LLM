{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "4aafa2b7-427a-4dde-a0fb-9dcbaf551bf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9.1\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import *\n",
    "import time \n",
    "import os\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "print(torch.__version__)\n",
    "\n",
    "\n",
    "\n",
    "block_size = 8\n",
    "batch_size = 4\n",
    "max_iters = 1000\n",
    "# intervals = 2000\n",
    "learning_rate = 3e-4\n",
    "eval_iters = 250 \n",
    "\n",
    "\n",
    "#checking the cpu and device \n",
    "device = ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "\n",
    "\n",
    "#so we are working with cpu here "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "7097fe44-7078-47ea-be22-9f011766b4d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "mps\n",
      "Its here \n"
     ]
    }
   ],
   "source": [
    "#device check if cpu is there or not :\n",
    "\n",
    "device = ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "\n",
    "#now since we are on apple we must also check and utilise that apple m2 chip...as they metal chip something\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "     device = torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "print(device)\n",
    "\n",
    "\n",
    "\n",
    "#Okayyyy Apple m2 chip in action babyyyyy\n",
    "\n",
    "#lets also check if torch.nn module is imported on not just to make sure:\n",
    "nn_module_check = (print(\"Its here \") if torch.nn else print(\"its not\")) #using python's ternary operator aka short hand if else here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "76417607-1e1d-4c13-9651-e448be8ad657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Here the data type is :<class 'str'>\n",
      "total length of the characters: 428055\n",
      "Deep Learning\n",
      "from Scratch\n",
      "Building with Python from First Principles\n",
      "\n",
      "Seth Weidman\n",
      "\n",
      "\f",
      "\f",
      "Deep Learning from Scratch\n",
      "\n",
      "Building with Python from First Principles\n",
      "\n",
      "Seth Weidman\n",
      "\n",
      "Beijing\n",
      "\n",
      "Boston Farnham Seb\n"
     ]
    }
   ],
   "source": [
    "#text corpus \n",
    "with open('O_Reillys_deepLearning.txt','r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "    print(f\" Here the data type is :{type(text)}\")\n",
    "    print(\"total length of the characters:\", len(text))\n",
    "    print(text[:200])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "c1fed763-615d-491a-a049-9f924e805744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lenght of characters in this book(deeplearning is : 120\n",
      "['\\n', '\\x0c', ' ', '!', '\"', '#', '%', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '<', '=', '>', '?', '@', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', '\\\\', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '{', '|', '}', '©', '×', 'é', 'Δ', 'Λ', 'α', 'β', 'δ', 'μ', 'ν', 'σ', 'ϵ', '‐', '–', '—', '’', '“', '”', '•', '…', '′', '∂', '∇', '−', '≅', '≈', '⋮', '�']\n"
     ]
    }
   ],
   "source": [
    "#now here we are making a sorted set of text :\n",
    "\n",
    "char = sorted(set(text))\n",
    "print(\"lenght of characters in this book(deeplearning is :\", len(char),  end='\\n')\n",
    "print(char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "a66d12cf-380a-4b1c-af4c-93df6fa20baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now as we have a huge set of array ,\n",
    "# we need an ecoder to make them into numbers which then we have use it sequence , because well , thats' how \n",
    "# tokensiers and llm work lol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "b7eb49df-4420-4800-a671-1e05b8f50bff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[70, 67, 74, 74, 77]\n",
      "hello\n"
     ]
    }
   ],
   "source": [
    "#conversion \n",
    "\n",
    "# string_to_int = {char[i]: i for i in range(len(char))}\n",
    "# int_to_string = {i:char[i]  for i in range(len(char))}\n",
    "\n",
    "string_to_int = {ch:i for i,ch in enumerate(char)}\n",
    "int_to_string = {i:ch for i,ch in enumerate(char)}\n",
    "\n",
    "encode = lambda s: [string_to_int[c] for c in s]\n",
    "decode = lambda d: ''.join([int_to_string[k] for k in d])\n",
    "\n",
    "# Another method of doing it :\n",
    "\n",
    " \n",
    "encoded_hello = encode(\"hello\")\n",
    "print(encoded_hello)\n",
    "print(decode(encoded_hello))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "333e48b5-fbdd-40c9-bc1b-efa388ffe4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now testing \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "9159a405-c37b-4826-b095-4bee0263948b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([36, 67, 67, 78,  2, 44, 67, 63, 80, 76, 71, 76, 69,  0, 68, 80, 77, 75,\n",
      "         2, 51, 65, 80, 63, 82, 65, 70,  0, 34, 83, 71, 74, 66, 71, 76, 69,  2,\n",
      "        85, 71, 82, 70,  2, 48, 87, 82, 70, 77, 76,  2, 68, 80, 77, 75,  2, 38,\n",
      "        71, 80, 81, 82,  2, 48, 80, 71, 76, 65, 71, 78, 74, 67, 81,  0,  0, 51,\n",
      "        67, 82, 70,  2, 55, 67, 71, 66, 75, 63, 76,  0,  0,  1,  1, 36, 67, 67,\n",
      "        78,  2, 44, 67, 63, 80, 76, 71, 76, 69])\n"
     ]
    }
   ],
   "source": [
    "data = torch.tensor(encode(text),dtype = torch.long)\n",
    "print(data[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "6a7b0229-d484-4422-b783-fff50e17da0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now as we learned from our machine learing classes , \n",
    "# we need to put data into : train , test, validate categories ,\n",
    "# 80% goes in for train  & 20% goes in for test so "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "ffa04071-04b2-40a3-b758-8cad4abcfff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# n = int(0.8 * len(data))\n",
    "# train_split = data[:n]\n",
    "# test_split = data[n:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "270e8104-7bb9-4bbe-9165-70a10411b3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# block_size = 8\n",
    "#testing\n",
    "# x = train\n",
    "# y = test\n",
    "\n",
    "# for block_c in range(block_size):\n",
    "#     context = x[:block_c+1]\n",
    "#     target = y[block_c]\n",
    "#     print(f\"when input is {context}  target is :{target}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "334c8928-f24e-4e60-9e19-b22ceebd0bc9",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'torch.dtype' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[321]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m n = \u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[32;43m0.8\u001b[39;49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m train_data = data[:n]\n\u001b[32m      3\u001b[39m val_data = data[n:]\n",
      "\u001b[31mTypeError\u001b[39m: 'torch.dtype' object is not callable"
     ]
    }
   ],
   "source": [
    "n = int(0.8*len(data)) #aqi yo ..I dividing into percentage(80% of data goes into train and 20% goes into val)\n",
    "train_data = data[:n] #esto para traning \n",
    "val_data = data[n:]  #esto para validation\n",
    "\n",
    "# def get_batch(split):\n",
    "#     data = train_data if split == 'train' else val_data\n",
    "#     ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "#     x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "#     y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "#     x, y = x.to(device), y.to(device)\n",
    "#     return x, y\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data)-block_size,(batch_size)) #mention above in the top cell \n",
    "    x = torch.stack([data[i:i+block_size+1] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1]for i in ix])\n",
    "\n",
    "x, y = get_batch('train')\n",
    "print('inputs:')\n",
    "# print(x.shape)\n",
    "print(x)\n",
    "print('targets:')\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b75db2-2630-42fd-8376-0b1bdfe33ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b48334-0ef0-4994-85f9-1f5e82260b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Langauge_model(nn.Module):\n",
    "    def __init__(self,vocab_size):\n",
    "        super().__init__()  #Gaby this is class inhertance !\n",
    "        self.token_emebeding_table = nn.Embedding(vocab_size,vocab_size)\n",
    "        #now we are creating a class method here:\n",
    "        \n",
    "        def forware(self,index,targets=None):\n",
    "            logits = self.token_embedding_tabel(index)\n",
    "\n",
    "            if targets is None:  \n",
    "                loss = None\n",
    "            else:\n",
    "                B,T,C = logits.shape\n",
    "                logits = logits.view(B*T,C)\n",
    "                targets = targets.view(B*T)\n",
    "                loss = F.cross_entrophy(logits,targets)\n",
    "                \n",
    "            return  logits,loss\n",
    "\n",
    "        def generate(self,index,max_new_tokens):\n",
    "            for _ in range(max_new_tokens):\n",
    "                logits,loss = self.forward(index)\n",
    "                logits = logits[:,-1,:]\n",
    "                #applying softmax to get probalities\n",
    "                probability = F.softmax(logits, dim= 1)\n",
    "                index_insert = torch.multinomial(probability,num_samples=1)\n",
    "                index = torch.cat((index, index_insert),dim = 1)\n",
    "            return index\n",
    "\n",
    "model = Langauge_model\n",
    "m = model.to(device)\n",
    "\n",
    "context = torch.zeros((1,1), dtype=torch.long, device=device)\n",
    "generated_chars = decode(m.generate(context, max_new_tokens=500)[0].tolist())\n",
    "print(generated_chars)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                \n",
    "\n",
    "#is used when you want to check if two variables refer to the exact same object, not just equal values.\n",
    "#and == when you want to check whether two things have the same value, \n",
    " #even if they are stored in different places in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd186b3-7217-4b32-9765-c28a5ee81a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "    if iter % eval_iters == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step: {iter}, train loss: {losses['train']:.3f}, val loss: {losses['val']:.3f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model.forward(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046894fe-a981-4f49-bb1f-3c4dd913b946",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = torch.zeros((1,1), dtype=torch.long, device=device)\n",
    "generated_chars = decode(m.generate(context, max_new_tokens=500)[0].tolist())\n",
    "print(generated_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b314e2a2-a4d0-4001-9869-782321389d67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261ab965-dd8c-4544-b158-c86ea9d38f2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7fe7d14-4827-4ffd-ba2e-1a427e1851f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5d434e-257b-4f38-a5d2-b0c1d64265f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3242ee84-4252-4fec-b04f-881f01f05b07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40fbe9b-2a4c-4f98-80f3-3c4a2e5d778d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b2a1bb-a80d-42d8-8b3a-c9c006783561",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
